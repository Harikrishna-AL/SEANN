{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iHgaEJKIH7xL"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "\n",
        "\n",
        "# import NMIST data\n",
        "def get_data(batch_size=128):\n",
        "    train_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "    test_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "    # truncate the remaining data that doesn't make a full batch\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# make a function that gives a mnist dataloader that gives a continous data of only classes 0-4 and after that 5-9\n",
        "def get_data_separate(batch_size=128):\n",
        "    train_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "    test_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "\n",
        "    train_data_1 = []\n",
        "    train_data_2 = []\n",
        "    test_data_1 = []\n",
        "    test_data_2 = []\n",
        "    for data, target in train_data:\n",
        "        if target < 5:\n",
        "            train_data_1.append((data, target))\n",
        "        else:\n",
        "            train_data_2.append((data, target))\n",
        "    for data, target in test_data:\n",
        "        if target < 5:\n",
        "            test_data_1.append((data, target))\n",
        "        else:\n",
        "            test_data_2.append((data, target))\n",
        "\n",
        "    train_loader_1 = torch.utils.data.DataLoader(\n",
        "        train_data_1, batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    train_loader_2 = torch.utils.data.DataLoader(\n",
        "        train_data_2, batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    test_loader_1 = torch.utils.data.DataLoader(\n",
        "        test_data_1, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "    test_loader_2 = torch.utils.data.DataLoader(\n",
        "        test_data_2, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    return train_loader_1, train_loader_2, test_loader_1, test_loader_2, test_loader\n",
        "\n",
        "def get_domain_inc_data(batch_size=128):\n",
        "    train_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "    test_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=torchvision.transforms.ToTensor(),\n",
        "    )\n",
        "\n",
        "    transforms = torchvision.transforms.Compose(\n",
        "        [   torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.RandomRotation(10),\n",
        "            torchvision.transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "            torchvision.transforms.RandomAffine(0, shear=10),\n",
        "            torchvision.transforms.RandomAffine(0, scale=(0.8, 1.2)),\n",
        "            # add random noise\n",
        "            torchvision.transforms.Lambda(lambda x: x + 0.01 * torch.randn_like(x)),\n",
        "         ]\n",
        "    )\n",
        "\n",
        "    transformed_train_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms,\n",
        "    )\n",
        "\n",
        "    transformed_test_data = torchvision.datasets.MNIST(\n",
        "        root=\"../../data\",\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms,\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    transformed_train_loader = torch.utils.data.DataLoader(\n",
        "        transformed_train_data, batch_size=batch_size, shuffle=True, drop_last=True\n",
        "    )\n",
        "    transformed_test_loader = torch.utils.data.DataLoader(\n",
        "        transformed_test_data, batch_size=batch_size, shuffle=False, drop_last=True\n",
        "    )\n",
        "\n",
        "    return train_loader, transformed_train_loader, test_loader, transformed_test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class NN(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network class with Hebbian learning mechanisms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, indexes, inhibition_strength=0.01):\n",
        "        \"\"\"\n",
        "        Initializes the network layers, Hebbian parameters, and hooks for gradient freezing.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            output_size (int): Size of the output layer.\n",
        "            indexes (list): List of neuron indices to freeze during gradient updates.\n",
        "        \"\"\"\n",
        "\n",
        "        super(NN, self).__init__()\n",
        "\n",
        "        self.k = 5\n",
        "        self.inhibition_strength = inhibition_strength\n",
        "        self.percent_winner = 0.5\n",
        "\n",
        "        self.linear = nn.ModuleList(\n",
        "            [\n",
        "                nn.Linear(input_size, 256),\n",
        "                nn.Linear(256, 128),\n",
        "                nn.Linear(128, 64),\n",
        "                nn.Linear(64, output_size),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Define the Hebbian parameters corresponding to each layer\n",
        "        self.hebb_params = nn.ModuleList(\n",
        "            [\n",
        "                nn.Linear(input_size, 256, bias=False),\n",
        "                nn.Linear(256, 128, bias=False),\n",
        "                nn.Linear(128, 64, bias=False),\n",
        "                nn.Linear(64, output_size, bias=False),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i, heb_param in enumerate(self.hebb_params):\n",
        "            nn.init.kaiming_normal_(heb_param.weight)\n",
        "            heb_param.weight.requires_grad = False\n",
        "\n",
        "        self.indexes = indexes\n",
        "        self.hidden_size_array = [256, 128, 64, output_size]\n",
        "\n",
        "        if indexes != [[], [], []]:\n",
        "            self._register_gradient_hooks(self.indexes)\n",
        "\n",
        "    def forward(self, x, scalers=None, indexes=None, masks=None, indices_old=None, target=None):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "            indexes (list, optional): New indexes to update for freezing gradients.\n",
        "            masks (list, optional): Masking values applied to specific layers during forward pass.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor.\n",
        "            list: Hebbian scores for each layer.\n",
        "            list: Hebbian masks for each layer.\n",
        "        \"\"\"\n",
        "\n",
        "        if scalers is not None:\n",
        "            self.update_indexes(scalers)\n",
        "\n",
        "        hebbian_scores = []\n",
        "        hebbian_masks = []\n",
        "        hebbian_indices = []\n",
        "\n",
        "        for i, layer in enumerate(self.linear):\n",
        "            x1 = layer(x)\n",
        "            is_final_layer = (i == len(self.linear) - 1)\n",
        "\n",
        "            if masks is not None: # check later why multiplying the mask of the last layer as well causes a drop is accuracy values.\n",
        "                x1 = torch.mul(x1, masks[i])\n",
        "\n",
        "            x1 = F.relu(x1) if not is_final_layer else x1\n",
        "\n",
        "            hebbian_score, hebbian_index, hebbian_mask = self.hebbian_update(\n",
        "                x, x1, i, indices_old=indices_old[i], target=target if is_final_layer else None\n",
        "            )\n",
        "\n",
        "            hebbian_scores.append(hebbian_score)\n",
        "            hebbian_masks.append(hebbian_mask)\n",
        "            hebbian_indices.append(hebbian_index)\n",
        "\n",
        "            x = x1\n",
        "\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "\n",
        "        return x, hebbian_scores, hebbian_indices, hebbian_masks\n",
        "\n",
        "    def hebb_forward(self, x, indexes=None):\n",
        "        hebbian_scores = []\n",
        "        hebbian_masks = []\n",
        "\n",
        "        for i, layer in enumerate(self.hebb_params):\n",
        "            x1 = layer(x)\n",
        "            x1 = F.relu(x1)\n",
        "            if i < len(self.linear) - 1:\n",
        "                if indexes is not None:\n",
        "                    hebbian_score, hebbian_mask = self.hebbian_update(\n",
        "                        x, x1, i, indices_old=indexes[i]\n",
        "                    )\n",
        "                else:\n",
        "                    hebbian_score, hebbian_mask = self.hebbian_update(x, x1, i)\n",
        "                hebbian_scores.append(hebbian_score)\n",
        "                hebbian_masks.append(hebbian_mask)\n",
        "            x = x1\n",
        "\n",
        "        return x, hebbian_scores, hebbian_masks\n",
        "\n",
        "    def hebbian_update(self, x, y, layer_idx, lr=0.00005, threshold=0.5, indices_old=None, target=None):\n",
        "        \"\"\"\n",
        "        Calculates Hebbian-derived scores, masks, and scales for a layer.\n",
        "        Handles final layer differently using the one-hot target.\n",
        "        \"\"\"\n",
        "\n",
        "        gd_layer = self.linear[layer_idx]\n",
        "        x_size = self.hidden_size_array[layer_idx] # Size of the output dimension of the layer\n",
        "        # input_size = gd_layer.weight.size(1) # Size of the input dimension of the layer (x's features)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Check if this is the final layer (or equivalently, if target is provided)\n",
        "        is_final_layer = (target is not None) # Assuming target is only non-None for the final layer\n",
        "\n",
        "        # --- Calculate delta_w using appropriate rule (Unsupervised for hidden, Supervised for final) ---\n",
        "        if not is_final_layer:\n",
        "            # Using raw y for delta_w calculation as in your last version\n",
        "            post_T = y.t() # Shape: (output_size, batch_size)\n",
        "            pre = x        # Shape: (batch_size, input_size)\n",
        "\n",
        "            y_x = torch.mm(post_T, pre) / batch_size # Shape: (output_size, input_size) - Pre-post correlation average\n",
        "\n",
        "            y_y_T = torch.mm(post_T, y) / batch_size # Shape: (output_size, output_size) - Post-post correlation average\n",
        "            # Applying lower triangle for Oja-like / competitive term\n",
        "            heb_mask_tril = torch.tril(torch.ones(y_y_T.size(), device=y_y_T.device))\n",
        "            y_y_T_lower = y_y_T * heb_mask_tril\n",
        "\n",
        "            # Lateral term using current linear weights\n",
        "            lateral_term = torch.mm(y_y_T_lower, gd_layer.weight.data) # Shape: (output_size, input_size)\n",
        "\n",
        "            # Hebbian weight update delta\n",
        "            delta_w = lr * (y_x - lateral_term)\n",
        "\n",
        "            modified_weights = gd_layer.weight.data + delta_w\n",
        "            with torch.no_grad():\n",
        "                 # Normalize rows (incoming weights for each output neuron)\n",
        "                 norm = torch.norm(modified_weights, p=2, dim=1, keepdim=True) # Shape: (output_size, 1)\n",
        "                 norm = torch.clamp(norm, min=1e-8) # Avoid division by zero\n",
        "                 normalized_modified_weights = modified_weights / norm\n",
        "\n",
        "\n",
        "            # Score for each output neuron is the norm of its incoming weight vector\n",
        "            hebbian_scores = torch.norm(normalized_modified_weights.detach(), p=2, dim=1) # Shape: (output_size)\n",
        "\n",
        "            # Apply inhibition from old indices to Hebbian scores before selecting top K\n",
        "            if indices_old is not None:\n",
        "                 # scatter expects index to be long tensor\n",
        "                 hebbian_scores = hebbian_scores.scatter(0, indices_old.long(), float('-inf'))\n",
        "\n",
        "            # Select top K based on Hebbian scores\n",
        "            num_winners = int(self.percent_winner * x_size)\n",
        "            if num_winners == 0 and x_size > 0: num_winners = 1 # Ensure at least one winner if layer exists\n",
        "            elif x_size == 0: num_winners = 0 # Handle empty layer gracefully\n",
        "\n",
        "            if num_winners > 0:\n",
        "                 # topk_indices_hebbian contains the indices (position in the layer's output dimension)\n",
        "                 _, topk_indices_hebbian = torch.topk(hebbian_scores, num_winners) # Shape: (num_winners)\n",
        "            else:\n",
        "                 topk_indices_hebbian = torch.tensor([], dtype=torch.long, device=y.device)\n",
        "\n",
        "\n",
        "            # Create the Hebbian-based winner mask (shape: 1, output_size) for activation masking\n",
        "            # This mask will be applied in the *next* forward pass for this task\n",
        "            hebbian_mask = torch.zeros(1, x_size, device=y.device)\n",
        "            if num_winners > 0:\n",
        "                 # Scatter needs index dimension to match self dimension (dim=1 here)\n",
        "                 hebbian_mask.scatter_(1, topk_indices_hebbian.unsqueeze(0), 1.0) # Indices need shape (1, num_winners)\n",
        "\n",
        "            # Get the indices of the non-selected neurons (for indices_old in next iter)\n",
        "            all_indices = torch.arange(x_size, device=y.device)\n",
        "            indices_non_winners = all_indices[hebbian_mask.squeeze(0) == 0] # Select indices where mask is 0\n",
        "\n",
        "            scale = torch.zeros_like(gd_layer.weight.data) # Shape: (output_size, input_size)\n",
        "\n",
        "            if num_winners > 0:\n",
        "                 scale[topk_indices_hebbian] = normalized_modified_weights[topk_indices_hebbian]\n",
        "\n",
        "            return scale, indices_non_winners, hebbian_mask\n",
        "\n",
        "\n",
        "        else:\n",
        "            x_size = self.hidden_size_array[layer_idx] # Size of the output dimension of the layer\n",
        "            if target is None:\n",
        "                 print(\"Warning: Target is None for final layer Hebbian update.\")\n",
        "                 scale_output = torch.zeros_like(gd_layer.weight.data)\n",
        "                 hebbian_mask = torch.ones(1, x_size, device=y.device)\n",
        "                 indices_non_winners = torch.tensor([], dtype=torch.long, device=y.device)\n",
        "                 return scale_output, indices_non_winners, hebbian_mask\n",
        "\n",
        "            target_onehot = target\n",
        "            post_T_supervised = target_onehot.t() # Shape: (output_size, batch_size)\n",
        "            pre_supervised = x                     # Shape: (batch_size, input_size)\n",
        "\n",
        "            # Correlation term averaged over batch\n",
        "            correlation_term = torch.mm(post_T_supervised, pre_supervised) / batch_size # Shape: (output_size, input_size)\n",
        "\n",
        "            scale_output = correlation_term.detach() # Shape: (output_size, input_size)\n",
        "            # Normalize scale for gradients between 0 and 1 (optional, but good practice)\n",
        "            min_scale = torch.min(scale_output)\n",
        "            max_scale = torch.max(scale_output)\n",
        "            if max_scale - min_scale > 1e-8:\n",
        "                 scale_output = (scale_output - min_scale) / (max_scale - min_scale)\n",
        "\n",
        "            hebbian_scores = torch.norm(scale_output, p=2, dim=1) # Shape: (output_size)\n",
        "            if indices_old is not None:\n",
        "                hebbian_scores = hebbian_scores.scatter(0, indices_old.long(), float('-inf'))\n",
        "            _, topk_indices_hebbian = torch.topk(hebbian_scores, int(self.percent_winner * x_size)) # Shape: (1)\n",
        "            hebbian_mask = torch.zeros(1, x_size, device=y.device)\n",
        "            hebbian_mask.scatter_(1, topk_indices_hebbian.unsqueeze(0), 1.0)\n",
        "            indices_non_winners = torch.arange(x_size, device=y.device)[hebbian_mask.squeeze(0) == 0] # Select indices where mask is 0\n",
        "\n",
        "            return scale_output, indices_non_winners, hebbian_mask\n",
        "\n",
        "    def scale_grad(self, scalers):\n",
        "        \"\"\"\n",
        "        Scales gradients for neurons specified by indexes.\n",
        "\n",
        "        Args:\n",
        "            indexes (list): List of neuron indices to scale during gradient updates.\n",
        "\n",
        "        Returns:\n",
        "            function: Hook function for modifying gradients during backpropagation.\n",
        "        \"\"\"\n",
        "\n",
        "        def hook(grad):\n",
        "            if len(scalers) > 0:\n",
        "                grad *= scalers\n",
        "            return grad\n",
        "\n",
        "        return hook\n",
        "\n",
        "    def freeze_grad(self, indexes):\n",
        "        \"\"\"\n",
        "        Freezes gradients for neurons specified by indexes.\n",
        "\n",
        "        Args:\n",
        "            indexes (list): List of neuron indices to freeze during gradient updates.\n",
        "\n",
        "        Returns:\n",
        "            function: Hook function for modifying gradients during backpropagation.\n",
        "        \"\"\"\n",
        "\n",
        "        def hook(grad):\n",
        "            if len(indexes) > 0:\n",
        "                indexes_arr = (\n",
        "                    indexes.cpu().numpy()\n",
        "                    if isinstance(indexes, torch.Tensor)\n",
        "                    else indexes\n",
        "                )\n",
        "                grad[indexes_arr] = 0\n",
        "            return grad\n",
        "\n",
        "        return hook\n",
        "\n",
        "    def _register_gradient_hooks(self, indexes):\n",
        "        \"\"\"\n",
        "        Registers hooks for freezing gradients on specified neurons.\n",
        "\n",
        "        Args:\n",
        "            indexes (list): List of neuron indices to freeze during gradient updates.\n",
        "        \"\"\"\n",
        "        for i, layer in enumerate(self.linear):\n",
        "            # Check if the layer already has hooks registered and clear them if they exist\n",
        "            if layer.weight._backward_hooks is not None:\n",
        "                layer.weight._backward_hooks.clear()\n",
        "            layer.weight.register_hook(self.scale_grad(indexes[i]))\n",
        "\n",
        "    def update_indexes(self, new_indexes):\n",
        "        \"\"\"\n",
        "        Updates the indexes of neurons for freezing and re-registers gradient hooks.\n",
        "\n",
        "        Args:\n",
        "            new_indexes (list): New list of neuron indexes to freeze.\n",
        "        \"\"\"\n",
        "\n",
        "        self.indexes = new_indexes\n",
        "        self._register_gradient_hooks(new_indexes)\n",
        "\n",
        "    def reinitialize_hebbian_parameters(self, init_type=\"zero\"):\n",
        "        \"\"\"\n",
        "        Reinitializes the Hebbian parameters.\n",
        "\n",
        "        Args:\n",
        "            init_type (str, optional): Initialization type ('zero' or 'normal').\n",
        "        \"\"\"\n",
        "\n",
        "        for param in self.hebb_params.parameters():\n",
        "            if init_type == \"zero\":\n",
        "                nn.init.constant_(param, 0)\n",
        "            elif init_type == \"normal\":\n",
        "                nn.init.kaiming_normal_(param)"
      ],
      "metadata": {
        "id": "rdLewjjNIJJx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "def get_excess_neurons(indices1, indices2, layer_sizes=[256, 128, 64]):\n",
        "    \"\"\"\n",
        "    Identifies neurons in each layer that are present in indices2 but not in indices1.\n",
        "\n",
        "    Args:\n",
        "        indices1 (list of lists): Indices of neurons selected for a particular task or layer.\n",
        "        indices2 (list of lists): Indices of neurons for comparison with indices1.\n",
        "        layer_sizes (list, optional): List of neuron counts per layer.\n",
        "\n",
        "    Returns:\n",
        "        list of torch.Tensor: List of indices representing neurons not present in either indices1 or indices2.\n",
        "    \"\"\"\n",
        "\n",
        "    # layer_sizes = [6,6,6]\n",
        "    excess_neurons = []\n",
        "    for i in range(len(indices1)):\n",
        "        excess_neurons.append([j for j in indices2[i] if j not in indices1[i]])\n",
        "\n",
        "    all_indices = [torch.arange(layer_sizes[i]) for i in range(len(layer_sizes))]\n",
        "\n",
        "    if excess_neurons == [[], [], []]:\n",
        "        for i in range(len(all_indices)):\n",
        "            # delete the indicces present in indices2 from all_indices\n",
        "            all_indices[i] = torch.tensor(\n",
        "                [j for j in all_indices[i] if j not in indices2[i]]\n",
        "            )\n",
        "        return all_indices\n",
        "\n",
        "    for i in range(len(indices1)):\n",
        "        all_indices[i] = torch.tensor(\n",
        "            [j for j in all_indices[i] if j not in excess_neurons[i]]\n",
        "        )\n",
        "\n",
        "    return all_indices\n",
        "\n",
        "\n",
        "def get_merge_mask(mask1, mask2):\n",
        "    \"\"\"\n",
        "    Merges two sets of binary masks using logical OR operation.\n",
        "\n",
        "    Args:\n",
        "        mask1 (list of torch.Tensor): First list of masks.\n",
        "        mask2 (list of torch.Tensor): Second list of masks.\n",
        "\n",
        "    Returns:\n",
        "        list of torch.Tensor: List of merged masks, where each mask is the result of logical OR operation.\n",
        "    \"\"\"\n",
        "\n",
        "    merge_mask = []\n",
        "    for i in range(len(mask1)):\n",
        "        merge_mask.append(torch.logical_or(mask1[i], mask2[i]).int())\n",
        "    return merge_mask\n",
        "\n",
        "\n",
        "def calc_percentage_of_zero_grad(masks):\n",
        "    \"\"\"\n",
        "    Calculates the percentage of neurons with non-zero gradients in the given masks.\n",
        "\n",
        "    Args:\n",
        "        masks (list of torch.Tensor): List of masks for each layer.\n",
        "\n",
        "    Returns:\n",
        "        float: Percentage of neurons with non-zero gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    total = 0\n",
        "    zero = 0\n",
        "    for mask in masks:\n",
        "        total += mask.numel()\n",
        "        zero += torch.sum(mask == 0).item()\n",
        "    return (1 - zero / total) * 100\n",
        "\n",
        "\n",
        "def forwardprop_and_backprop(\n",
        "    model,\n",
        "    lr,\n",
        "    data_loader,\n",
        "    continual=False,\n",
        "    list_of_indexes=None,\n",
        "    masks=None,\n",
        "    scheduler=None,\n",
        "    optimizer=None,\n",
        "    indices_old = None,\n",
        "    task_id=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs forward and backward propagation over a dataset with optional continual learning.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Neural network model.\n",
        "        lr (float): Learning rate for optimizer.\n",
        "        data_loader (DataLoader): DataLoader for the training data.\n",
        "        continual (bool, optional): Flag indicating whether continual learning is applied.\n",
        "        list_of_indexes (list, optional): List of indexes for selective neuron training.\n",
        "        masks (list, optional): List of masks for each layer.\n",
        "        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler.\n",
        "        optimizer (torch.optim.Optimizer, optional): Optimizer for the model.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Updated list of indexes, masks, model, and optimizer after training.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if optimizer is None:\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    loss_total = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(tqdm(data_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        data = data.view(-1, 784)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        scalers = None\n",
        "        one_hot_target = torch.zeros(target.size(0), 10).to(device)\n",
        "        one_hot_target.scatter_(1, target.view(-1, 1), 1)\n",
        "\n",
        "        if not continual:\n",
        "            indices_old = [None] * len(list_of_indexes)\n",
        "\n",
        "            output, scalers, list_of_indexes, masks = model(\n",
        "                data, scalers, indexes=list_of_indexes, masks=masks, indices_old = indices_old, target=one_hot_target\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            output, scalers, list_of_indexes, masks = model(\n",
        "                data,\n",
        "                scalers,\n",
        "                indexes=list_of_indexes,\n",
        "                masks=masks,\n",
        "                indices_old=indices_old,\n",
        "                target=one_hot_target,\n",
        "            )\n",
        "\n",
        "        # if task_id is not None:\n",
        "        #     output = output[:, 5*(task_id-1):5*task_id]\n",
        "        #     target = target % 5\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_total += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\"Avg loss: \", loss_total / len(data_loader))\n",
        "    return list_of_indexes, masks, model, optimizer"
      ],
      "metadata": {
        "id": "CVnUNz3YI22W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "seed = 924  # verified\n",
        "print(\"Seed: \", seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "data_loader_1, data_loader_2, test_loader_1, test_loader_2, test_loader = get_data_separate(\n",
        "    batch_size=64\n",
        ")\n",
        "list_of_indexes = [[], [], [],[]]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "masks = [\n",
        "    torch.ones(256).to(device),\n",
        "    torch.ones(128).to(device),\n",
        "    torch.ones(64).to(device),\n",
        "    torch.ones(10).to(device),\n",
        "]\n",
        "\n",
        "\n",
        "original_model = NN(784, 10, indexes=list_of_indexes).to(device)\n",
        "optimizer = optim.SGD(original_model.parameters(), lr=0.1, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "for i in range(10):\n",
        "    task1_indices, task1_masks, task1_model, optimizer = forwardprop_and_backprop(\n",
        "        original_model,\n",
        "        0.1,\n",
        "        data_loader_1,\n",
        "        list_of_indexes=list_of_indexes,\n",
        "        masks=masks,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        task_id=1,\n",
        "    )\n",
        "    list_of_indexes = task1_indices\n",
        "    # print(\"percentage of zero gradients: \",calc_percentage_of_zero_grad(original_model))\n",
        "\n",
        "indices = []\n",
        "new_masks = []\n",
        "layer_sizes = [256, 128, 64, 10]\n",
        "for i in range(len(layer_sizes)):\n",
        "    indices.append(\n",
        "        torch.tensor(\n",
        "            [j for j in range(layer_sizes[i]) if j not in task1_indices[i]]\n",
        "        ).to(device)\n",
        "    )\n",
        "    mask = torch.tensor(\n",
        "        [1 if k in task1_indices[i] else 0 for k in range(layer_sizes[i])]\n",
        "    ).to(device)\n",
        "    new_masks.append(mask)\n",
        "\n",
        "print(\"Task 1 indices: \", task1_indices)\n",
        "print(\"Task 1 masks: \", task1_masks)\n",
        "print(\"Percentage of frozen neurons: \", calc_percentage_of_zero_grad(task1_masks))\n",
        "\n",
        "print(\"### Task 2 ###\")\n",
        "for i in range(10):\n",
        "    task2_indices, task2_masks, task2_model, optimizer = forwardprop_and_backprop(\n",
        "        task1_model,\n",
        "        0.1,\n",
        "        data_loader_2,\n",
        "        list_of_indexes=task1_indices,\n",
        "        masks=new_masks,\n",
        "        continual=True,\n",
        "        optimizer=None,\n",
        "        scheduler=scheduler,\n",
        "        indices_old=indices,\n",
        "        task_id=2,\n",
        "    )\n",
        "# print(\"Percentage of frozen neurons: \", calc_percentage_of_zero_grad(task2_masks))\n",
        "# print(\"percentage of zero gradients: \",calc_percentage_of_zero_grad(original_model))\n",
        "\n",
        "print(\"Task 2 indices: \", task2_indices)\n",
        "print(\"Task 2 masks: \", task2_masks)\n",
        "print(\"Percentage of frozen neurons: \", calc_percentage_of_zero_grad(task2_masks))\n",
        "\n",
        "all_masks = [task1_masks, task2_masks]\n",
        "correct = 0\n",
        "accuracies = []\n",
        "original_model.eval()\n",
        "\n",
        "print(\"### Testing both Tasks ###\")\n",
        "\n",
        "\n",
        "\n",
        "def mc_entropy_weighted_ensemble(model, data, masks, mc_runs=10, temperature=1.0, device='cuda'):\n",
        "    \"\"\"\n",
        "    Do MC Dropout forward passes for each subnetwork, compute predictive entropy,\n",
        "    then fuse outputs by softmax of negative entropy (lower entropy → higher weight).\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    batch_size = data.size(0)\n",
        "    n_models = len(masks)\n",
        "    all_probs = torch.zeros(n_models, mc_runs, batch_size, model.linear[-1].out_features, device=device)\n",
        "\n",
        "\n",
        "    for m, mask in enumerate(masks):\n",
        "        for t in range(mc_runs):\n",
        "            with torch.no_grad():\n",
        "                out, *_ = model(data, masks=mask, indices_old=[None]*len(mask))\n",
        "            all_probs[m, t] = out\n",
        "\n",
        "    # average to get predictive p̂(x)\n",
        "    p_mc = all_probs.mean(dim=1)\n",
        "\n",
        "    # compute entropy for each model & sample\n",
        "    entropies = -torch.sum(p_mc * torch.log(p_mc + 1e-10), dim=2)  # shape: (n_models, batch)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # lower entropy => higher weight\n",
        "    weights = F.softmax(-entropies / temperature, dim=0)\n",
        "    weights = weights.unsqueeze(2)\n",
        "\n",
        "    # 5) fuse the averaged probabilities\n",
        "    fused = torch.sum(weights * p_mc, dim=0)\n",
        "    return fused\n",
        "\n",
        "\n",
        "\n",
        "correct = 0\n",
        "original_model.eval()\n",
        "\n",
        "for data, target in test_loader:\n",
        "    data = data.view(-1, 784).to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    fused_probs = mc_entropy_weighted_ensemble(\n",
        "        original_model,\n",
        "        data,\n",
        "        masks=all_masks,\n",
        "        mc_runs=20,\n",
        "        temperature=0.5,\n",
        "        device=device,\n",
        "    )\n",
        "    preds = fused_probs.argmax(dim=1)\n",
        "    correct += preds.eq(target).sum().item()\n",
        "\n",
        "acc = 100.0 * correct / len(test_loader.dataset)\n",
        "print(f\"MC‐Dropout + Entropy‐Weighted Ensemble accuracy: {acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Accuracy for both Tasks: {100 * correct / len(test_loader.dataset):.2f}%\")\n",
        "\n",
        "correct = 0\n",
        "print(\"### Testing Task 1###\")\n",
        "task_id = 1\n",
        "for data, target in test_loader_1:\n",
        "    data = data.view(-1, 784)\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output, scalers, indices, masks = task1_model(data, masks=task1_masks, indices_old=[None]*len(indices))\n",
        "    # check the accuracy\n",
        "    predicted = output.argmax(dim=1, keepdim=True)\n",
        "    correct += predicted.eq(target.view_as(predicted)).sum().item()\n",
        "\n",
        "print(f\"Accuracy for Task 1: {100* correct/len(test_loader_1.dataset)}%\")\n",
        "accuracies.append(100 * correct / len(test_loader_1.dataset))\n",
        "\n",
        "# task2_masks = get_merge_mask(task1_masks, task2_masks)\n",
        "\n",
        "correct = 0\n",
        "print(\"### Testing Task 2###\")\n",
        "task_id = 2\n",
        "for data, target in test_loader_2:\n",
        "    data = data.view(-1, 784)\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output, scalers, indices, masks = task2_model(data, masks=task2_masks, indices_old=[None]*len(indices))\n",
        "    # check the accuracy\n",
        "    predicted = output.argmax(dim=1, keepdim=True)\n",
        "    # target = target % 5\n",
        "    correct += predicted.eq(target.view_as(predicted)).sum().item()\n",
        "\n",
        "print(f\"Accuracy for Task 2: {100* correct/len(test_loader_2.dataset)}%\")\n",
        "accuracies.append(100 * correct / len(test_loader_2.dataset))\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# hebbian_weights = task1_model.hebb_params[0].weight.data.cpu().numpy()\n",
        "# model_weights = task2_model.linear[0].weight.data.cpu().numpy()\n",
        "# model_weights1 = task2_model.linear[1].weight.data.cpu().numpy()\n",
        "\n",
        "# model_neurons = np.random.choice(256, 20)\n",
        "# model_neurons1 = np.random.choice(128, 20)\n",
        "# # select random 20 neurons\n",
        "# neurons = np.random.choice(256, 20)\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i, neuron in enumerate(neurons):\n",
        "#     plt.subplot(4, 5, i + 1)\n",
        "#     plt.imshow(hebbian_weights[neuron].reshape(28, 28), cmap=\"gray\")\n",
        "#     plt.axis(\"off\")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i, neuron in enumerate(model_neurons):\n",
        "#     idx = neuron\n",
        "#     # idx = task2_indices[0][neuron]\n",
        "#     plt.subplot(4, 5, i + 1)\n",
        "#     plt.imshow(model_weights[idx].reshape(28, 28), cmap=\"gray\")\n",
        "#     plt.axis(\"off\")\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i, neuron in enumerate(model_neurons1):\n",
        "#     idx = neuron\n",
        "#     # idx = task2_indices[1][neuron]\n",
        "#     plt.subplot(4, 5, i + 1)\n",
        "#     plt.imshow(model_weights1[idx].reshape(16, 16), cmap=\"gray\")\n",
        "#     plt.axis(\"off\")\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9_UjqWMI9oT",
        "outputId": "631155ca-5b21-4e17-ef97-9812ada7b70d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:  924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.71MB/s]\n",
            "100%|██████████| 478/478 [00:03<00:00, 133.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.9166911259355903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 219.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5095521478473393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 220.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.497603766838377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 217.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.490234434105861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 224.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.4888367388537738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 184.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.4823020477175213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 224.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.4802387399154726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 226.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.4796531128085308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 218.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.479382449114173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 478/478 [00:02<00:00, 222.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.478153832038576\n",
            "Task 1 indices:  [tensor([  2,   4,   9,  14,  16,  18,  20,  21,  22,  23,  24,  26,  27,  28,\n",
            "         31,  35,  36,  40,  42,  44,  47,  51,  57,  58,  63,  67,  69,  70,\n",
            "         71,  76,  78,  85,  89,  92,  93,  97, 101, 102, 104, 107, 108, 109,\n",
            "        111, 115, 117, 118, 121, 124, 126, 132, 133, 134, 135, 141, 142, 149,\n",
            "        150, 152, 168, 170, 172, 177, 178, 179, 180, 182, 183, 188, 189, 195,\n",
            "        196, 197, 198, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
            "        212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
            "        226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
            "        240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
            "        254, 255], device='cuda:0'), tensor([  1,   4,   7,   9,  10,  11,  15,  16,  17,  21,  23,  25,  26,  27,\n",
            "         28,  34,  47,  49,  50,  51,  58,  60,  61,  62,  64,  73,  75,  79,\n",
            "         82,  90,  93,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
            "        106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
            "        120, 121, 122, 123, 124, 125, 126, 127], device='cuda:0'), tensor([ 0,  2,  5,  9, 10, 11, 15, 20, 22, 26, 32, 34, 35, 41, 43, 44, 46, 49,\n",
            "        50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n",
            "       device='cuda:0'), tensor([5, 6, 7, 8, 9], device='cuda:0')]\n",
            "Task 1 masks:  [tensor([[1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
            "         0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
            "         1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
            "         1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
            "         1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
            "         0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
            "         0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
            "         1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
            "         0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0.]], device='cuda:0'), tensor([[1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "         1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
            "         1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]], device='cuda:0'), tensor([[0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
            "         1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
            "         1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0'), tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]], device='cuda:0')]\n",
            "Percentage of frozen neurons:  50.0\n",
            "### Task 2 ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 186.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.6643502107113275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 202.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5130411430641457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 217.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5052824493067456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 218.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5036155990525788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 160.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5011190970738728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 194.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.499105511667422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 198.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5034695907875344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 216.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.5018870962990656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 216.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.49945105420738\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 459/459 [00:02<00:00, 218.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg loss:  1.4993251021910856\n",
            "Task 2 indices:  [tensor([  0,   1,   3,   5,   6,   7,   8,  10,  11,  12,  13,  15,  17,  19,\n",
            "         25,  29,  30,  32,  33,  34,  37,  38,  39,  41,  43,  45,  46,  48,\n",
            "         49,  50,  52,  53,  54,  55,  56,  59,  60,  61,  62,  64,  65,  66,\n",
            "         68,  72,  73,  74,  75,  77,  79,  80,  81,  82,  83,  84,  86,  87,\n",
            "         88,  90,  91,  94,  95,  96,  98,  99, 100, 103, 105, 106, 110, 112,\n",
            "        113, 114, 116, 119, 120, 122, 123, 125, 127, 128, 129, 130, 131, 136,\n",
            "        137, 138, 139, 140, 143, 144, 145, 146, 147, 148, 151, 153, 154, 155,\n",
            "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 171,\n",
            "        173, 174, 175, 176, 181, 184, 185, 186, 187, 190, 191, 192, 193, 194,\n",
            "        199, 200], device='cuda:0'), tensor([ 0,  2,  3,  5,  6,  8, 12, 13, 14, 18, 19, 20, 22, 24, 29, 30, 31, 32,\n",
            "        33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 52, 53, 54, 55,\n",
            "        56, 57, 59, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 80, 81,\n",
            "        83, 84, 85, 86, 87, 88, 89, 91, 92, 94], device='cuda:0'), tensor([ 1,  3,  4,  6,  7,  8, 12, 13, 14, 16, 17, 18, 19, 21, 23, 24, 25, 27,\n",
            "        28, 29, 30, 31, 33, 36, 37, 38, 39, 40, 42, 45, 47, 48],\n",
            "       device='cuda:0'), tensor([0, 1, 2, 3, 4], device='cuda:0')]\n",
            "Task 2 masks:  [tensor([[0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
            "         1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "         1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
            "         0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
            "         0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
            "         1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
            "         1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
            "         1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1.]], device='cuda:0'), tensor([[0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
            "         0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "         1., 1.]], device='cuda:0'), tensor([[1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0'), tensor([[0., 0., 0., 0., 0., 1., 1., 1., 1., 1.]], device='cuda:0')]\n",
            "Percentage of frozen neurons:  50.0\n",
            "### Testing both Tasks ###\n",
            "Entropy-weighted ensemble accuracy: 69.20%\n",
            "MC‐Dropout + Entropy‐Weighted Ensemble accuracy: 69.13%\n",
            "Accuracy for both Tasks: 69.13%\n",
            "### Testing Task 1###\n",
            "Accuracy for Task 1: 98.17085035999222%\n",
            "### Testing Task 2###\n",
            "Accuracy for Task 2: 93.86957416169513%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#  Router (Actor-Critic\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_experts=2):\n",
        "        super().__init__()\n",
        "        self.fc1       = nn.Linear(input_dim, hidden_dim)\n",
        "        self.actor_fc  = nn.Linear(hidden_dim, n_experts)\n",
        "        self.critic_fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, input_dim)\n",
        "        h = F.relu(self.fc1(x))\n",
        "        logits = self.actor_fc(h)     # (batch, n_experts)\n",
        "        value  = self.critic_fc(h).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "\n",
        "def train_router(router,\n",
        "                 expert_model,\n",
        "                 masks,\n",
        "                 data_loader,\n",
        "                 n_epochs=5,\n",
        "                 gamma=0.99,\n",
        "                 lr=1e-3):\n",
        "    optimizer = optim.Adam(router.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        router.train()\n",
        "        epoch_reward = 0\n",
        "        for data, target in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
        "            data, target = data.view(-1,784).to(device), target.to(device)\n",
        "\n",
        "\n",
        "            logits, values = router(data)\n",
        "            dist    = Categorical(logits=logits)\n",
        "            actions = dist.sample()           # (batch,)\n",
        "            logp    = dist.log_prob(actions)  # (batch,)\n",
        "\n",
        "            # route through chosen expert, get per-sample reward\n",
        "            rewards = []\n",
        "            for i in range(data.size(0)):\n",
        "                x_i   = data[i:i+1]\n",
        "                a     = actions[i].item()\n",
        "                m     = masks[a]\n",
        "                out, *_ = expert_model(\n",
        "                    x_i,\n",
        "                    masks=m,                       # pass it directly\n",
        "                    indices_old=[None]*len(m)     # same length as layers\n",
        "                )\n",
        "                pred = out.argmax(dim=1)\n",
        "                rewards.append((pred == target[i]).float().item())\n",
        "\n",
        "            rewards = torch.tensor(rewards, device=device)  # (batch,)\n",
        "\n",
        "            # compute advantages (one‐step)\n",
        "            returns    = rewards\n",
        "            advantages = returns - values.detach()\n",
        "\n",
        "            # losses\n",
        "            policy_loss = -(logp * advantages).mean()\n",
        "            value_loss  = F.mse_loss(values, returns)\n",
        "            loss        = policy_loss + 0.5 * value_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_reward += rewards.sum().item()\n",
        "\n",
        "        avg_reward = epoch_reward / len(data_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}: Avg routing reward = {avg_reward:.4f}\")\n",
        "\n",
        "\n",
        "#  reuse  pretrained expert\n",
        "original_model.eval()\n",
        "for p in original_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "\n",
        "router = Router(input_dim=784, hidden_dim=128, n_experts=2).to(device)\n",
        "\n",
        "\n",
        "full_train = torch.utils.data.ConcatDataset([data_loader_1.dataset,\n",
        "                                              data_loader_2.dataset])\n",
        "full_loader = torch.utils.data.DataLoader(full_train,\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True)\n",
        "train_router(router, original_model, all_masks, full_loader,\n",
        "              n_epochs=10, gamma=0.99, lr=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "# evalutaion\n",
        "\n",
        "router.eval()\n",
        "correct = 0\n",
        "total   = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.view(-1,784).to(device), target.to(device)\n",
        "        logits, _   = router(data)\n",
        "        expert_ids  = logits.argmax(dim=1)  # pick the best expert\n",
        "        for i in range(data.size(0)):\n",
        "            x_i = data[i:i+1]\n",
        "            a   = expert_ids[i].item()\n",
        "            m   = all_masks[a]\n",
        "            out, *_ = original_model(\n",
        "                x_i,\n",
        "                masks=m,\n",
        "                indices_old=[None]*len(m)\n",
        "            )\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += int(pred == target[i])\n",
        "        total += data.size(0)\n",
        "\n",
        "acc = 100.0 * correct / total\n",
        "print(f\"RL-router ensemble accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "DOs8mQOcgreu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0229076e-5417-4797-da55-e5cba13010fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 938/938 [02:59<00:00,  5.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Avg routing reward = 0.8415\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 938/938 [02:58<00:00,  5.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Avg routing reward = 0.9182\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 938/938 [02:57<00:00,  5.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Avg routing reward = 0.9310\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 938/938 [02:57<00:00,  5.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Avg routing reward = 0.9381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 938/938 [02:57<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Avg routing reward = 0.9409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 938/938 [02:56<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Avg routing reward = 0.9449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 938/938 [02:57<00:00,  5.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Avg routing reward = 0.9464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 938/938 [02:57<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Avg routing reward = 0.9465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 938/938 [02:56<00:00,  5.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Avg routing reward = 0.9494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 938/938 [02:56<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Avg routing reward = 0.9506\n",
            "RL-router ensemble accuracy: 94.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lKPZUVkTaGVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}